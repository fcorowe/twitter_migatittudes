---
title: "Twitter Data Analysis - VADER"
author: "F Rowe"
output: html_notebook
---

```{r}
rm(list=ls())
```


# 1. Library necessary packages
```{r}
library(rtweet)
library(tidyverse)
library(tidytext)
library(textdata)
library(dplyr)
library(stringr)
library(tidyr)
library(plyr)
library(lubridate)
library(ggplot2)
library(RColorBrewer)
library(gridExtra)
library(sentimentr)
library(ggthemes)
library(showtext)
library(wordcloud)
library(vader)
library(patchwork)
library(viridis)
```

Set font style
```{r}
# load font
font_add_google("Roboto Condensed", "robotocondensed")
# automatically use showtext to render text
showtext_auto()
```


# 2. Data Wrangling
```{r}
# Read in tweets
tweets <- read_csv("../data/uk_tweets_01122019_01052020_VADER_removed_dpl_RT_and_status_id_only.csv")
glimpse(tweets)
```


Formatting dates
```{r}
# split string character date
date_df <- tweets$created_at %>% 
  str_split(" ", simplify = TRUE) %>% 
  as.data.frame()
date_df %>% head(5)

# create date variable
tweets$date <- ymd(date_df$V1)
  
rm(date_df)
```

```{r}
#remove
tweets$X1 <- NULL

# country id
tweets$cntry_id <- 1 # uk

```


## Select sample for this analysis
```{r}
tweets <- tweets %>% dplyr::filter(between(
  date, as.Date("2020-01-15"),
  as.Date("2020-02-15")
  )
  )
```

# 3. Sentiment Analysis

## 3.1. Getting the scores
```{r}
vader_sentiment <- vader_df(tweets$VADER_text)
```

Computing summary metrics
```{r}
#join data
df <- cbind(tweets$date, vader_sentiment, tweets$retweet_count, tweets$quoted_tweet, tweets$created_at, tweets$status_id)

df <- df %>% dplyr::rename(date = `tweets$date`,
                retweet_count = `tweets$retweet_count`, 
                quoted_tweet = `tweets$quoted_tweet`,
                over_tweet_sent = compound,
                hour = `tweets$created_at`,
                status_id = `tweets$status_id`)


# day level sentiment score - check
day_sent <- df %>% 
  group_by(date) %>% 
  dplyr::summarize(
    wave_sent = weighted.mean(over_tweet_sent, retweet_count),
    ave_sent = mean(over_tweet_sent)
    )

# add vars
df <- df %>% 
  mutate(pn_sent = 
           case_when(
           over_tweet_sent == 0 ~ 0,
           over_tweet_sent > 0 ~ 1,
           over_tweet_sent < 0 ~ 2
           ), # id for neutral, positive and negative sentiment
         pn5c_sent =
           case_when(
           over_tweet_sent >= -.05 & over_tweet_sent <= .05 ~ 3,
           over_tweet_sent < -.5 ~ 1,
           over_tweet_sent < -.05 & over_tweet_sent >= -.5 ~ 2,
           over_tweet_sent > .05 & over_tweet_sent <= .5 ~ 4,
           over_tweet_sent > .5 ~ 5,
           )
         )
```

Expanding the dataset 
```{r}
# create function to expand the data
apply_weightings <- function (x) {
  x <- x[rep(row.names(x), x$retweet_count), c('date', 'text','over_tweet_sent','retweet_count','status_id', 'hour')]
}

wdf <- apply_weightings(df)
```


## 3.2 Total number of tweets
```{r}
sum(df$retweet_count)
```

## 3.3 Numbers by day
```{r}
df %>% group_by(date) %>% 
  dplyr::summarise(sum = retweet_count) %>%
  ggplot(., aes(x = date, y = sum)) +
  #geom_line() +
  geom_smooth(method = "loess", se = FALSE, size=2, span = 0.15, color="#238A8DFF") +
  theme_tufte()
```


## 3.3. Distribution

Raw SS
```{r}
p1 <- ggplot(data=df) +
  geom_histogram(aes(x = over_tweet_sent, 
                   weight = retweet_count/sum(retweet_count)), 
                 binwidth = 0.05,
                 fill = "#440154FF",
                 alpha = 1) +
  #facet_grid(. ~ cntry_id) +
  theme_tufte() + 
  theme(text = element_text(family="robotocondensed",
                            size = 20)) +
  labs(x= "Tweet sentiment score",
       y = "Density")
```

Checking weights have been applied correctly
```{r}
ggplot(data=wdf) +
  geom_histogram(aes(x = over_tweet_sent), 
                 binwidth = 0.05,
                 fill = "#440154FF",
                 alpha = 1) +
  #facet_grid(. ~ cntry_id) +
  theme_tufte() + 
  theme(text = element_text(family="robotocondensed",
                            size = 20)) +
  labs(x= "Tweet sentiment score",
       y = "Density")
```


```{r}
p2 <- ggplot(wdf, aes(over_tweet_sent)) + 
  stat_ecdf(geom = "step",
            size = 2,
            colour = "#440154FF") +
  theme_tufte() + 
  theme(text = element_text(family="robotocondensed",
                            size = 20)) +
  labs(x= "Tweet sentiment score",
       y = "Cumulative density")
```

```{r}
png("../outputs/oss_dist_uk.png", units="in", width=10, height=7, res=300)
p1 + p2 + plot_layout(widths = c(1, 1))
dev.off()
```


```{r}
ggplot(data=df) +
  geom_density(aes(x = over_tweet_sent), 
               fill = "blue",
               alpha = 0.6) +
  #facet_grid(. ~ cntry_id) +
  theme_tufte()
```


Examining outliers
> Lots of the very high positive SSs involve tweets on having got Brexit done

```{r}
outlier <- df %>% dplyr::filter(over_tweet_sent > .55 | over_tweet_sent < -.55)
outlier %>% dplyr::select(over_tweet_sent, text) %>% head()
```

Examining ~ 0 SSs

Shall small SSs be categorised as 0?
```{r}
near_zero <- df %>% dplyr::filter(over_tweet_sent <= .05 & over_tweet_sent >= -.05)
near_zero %>% dplyr::select(over_tweet_sent, text) %>% head()
```

Raw SS removing neutral SSs (ie 0s)
```{r}
df %>% dplyr::filter(pn_sent | 0) %>% 
  ggplot() +
  geom_density(aes(x = over_tweet_sent,
                   weight = retweet_count/sum(retweet_count)),
               fill = "blue", 
               alpha = 0.6) +
  #facet_grid(. ~ cntry_id) +
  theme_tufte()
```

## 3.4. Frequencies

Frequency Neu (0), Pos (1), Neg (2)
```{r}
dplyr::count(x = df, pn_sent, wt = retweet_count)
```

Totals - 5 categories Neu (3), S Neg (1), Neg (2), Pos (4), S Pos (5),
```{r}
dplyr::count(x = df, pn5c_sent, wt = retweet_count)
```

Daily Frequency Neu (0), Pos (1), Neg (2)
```{r}
tb_counts <- df %>% dplyr::count(date, pn_sent, wt = retweet_count) %>% 
  spread(pn_sent, n)
tb_counts
```


Daily Share Neu (0), Pos (1), Neg (2)
```{r}
tb_counts[,2:4] / rowSums(tb_counts[,2:4]) * 100
```

Daily Frequency - 5 categories Neu (3), S Neg (1), Neg (2), Pos (4), S Pos (5)
```{r}
tb_counts5 <- df %>% dplyr::count(date, pn5c_sent, wt = retweet_count) %>% 
  spread(pn5c_sent, n)
tab <- cbind(tb_counts5[,1], (tb_counts5[,2:6] / rowSums(tb_counts5[,2:6]) * 100)) %>% 
  dplyr::rename(day = `tb_counts5[, 1]`) %>% 
  .[, c(1, 3, 4, 2, 5, 6)] %>% 
  gather(stance, percent, -day)

pcom <- ggplot(tab, aes(fill = stance, y=percent, x=day)) + 
    geom_bar(position="stack", stat="identity") + 
    theme_tufte() + 
    theme(text = element_text(family="robotocondensed",
                            size = 20),
          legend.position = "bottom") +
    scale_fill_manual(values = c("darkred","#d7191c", "#f7f7f7", "#2c7bb6", "darkblue"),
                      labels = c("Strongly Negative", "Negative", "Neutral", "Positive", "Strongly Positive")) +
  labs(x= "Day",
       y = "Percent") +
   ggtitle('b.')
    #scale_fill_viridis_d()
```



## 3.5 Tweet level analysis

SS Daily evolution - smoothed conditional mean
> Weighted data using `geom_smooth`: https://ggplot2-book.org/statistical-summaries.html
> ggplot annotation: https://ggplot2-book.org/annotations.html

```{r}
p1 <- ggplot(df, aes(x = hour, y = over_tweet_sent)) +
 geom_point(colour = "gray", alpha = 0.3, size = 1, shape=".") + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", size = .3) +
  geom_smooth(aes(weight = retweet_count), method = "loess", se = TRUE, size=2, span = 0.3, color="#440154FF") +
# facet_wrap(~ cntry_id, nrow = 5) + 
  theme_tufte() + 
  theme(text = element_text(family="robotocondensed",
                            size = 20)) +
  labs(x= "Day",
       y = "Tweet sentiment score")  +
   ggtitle('a.') #+
 # scale_y_continuous(limits = c(-.5, .5))
png("../outputs/oss_points_uk.png", units="in", width=10, height=10, res=300)
p1 / pcom + plot_layout(widths = c(1, 1))
dev.off()
```

Daily mean SS
```{r}
df %>% 
  group_by(date) %>% 
  dplyr::summarize(
    wave_sent = weighted.mean(over_tweet_sent, retweet_count)) %>% 
  ggplot(aes(x = date, y = wave_sent)) +
  geom_point(colour = "gray", alpha = 0.5) + 
  geom_line(size=2, color="#238A8DFF") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", size = .3) +
#  facet_wrap(~ cntry_id, nrow = 5) + 
  theme_tufte() + 
  labs(x= "Day",
       y = "Tweet sentiment score")
```


Daily median SS
```{r}
df %>% group_by(date) %>% 
  dplyr::summarise(median_day_sent = matrixStats::weightedMedian(over_tweet_sent, retweet_count)) %>% 
  ggplot(aes(x = date, y = median_day_sent)) +
  geom_point(colour = "gray", alpha = 0.5) + 
  geom_line(size=2, color="#238A8DFF") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", size = .3) +
#  facet_wrap(~ cntry_id, nrow = 5) + 
  theme_tufte() + 
  labs(x= "Day",
       y = "Tweet sentiment score")
```

## 3.7 Group analysis: Positive, Neutral & Negative

 Positive and negative sentiment scores
```{r}
p <- df %>% dplyr::filter(over_tweet_sent < -.05 | over_tweet_sent > .05) %>% 
  ggplot(aes(x = hour, y = over_tweet_sent, colour = as.factor(pn_sent))) +
  geom_hline(yintercept = 0, linetype="dashed", color = "red", size = .3) +
  geom_point(colour = "gray", alpha = 0.3, size = 1, shape=".") 

p <- p +  geom_smooth(aes(weight = retweet_count), method = "loess", se = TRUE, size=2, span = 0.3) +
  scale_color_manual(name = "Sentiment score", 
                     labels = c("Positive", "Negative"), 
                     values = c('darkblue', 'darkred')) +
#  facet_wrap(~ cntry_id, nrow = 5) + 
  theme_tufte() + 
   theme(text = element_text(family="robotocondensed",
                            size = 20)) +
  theme(legend.position = "bottom") +
  labs(x= "Day",
       y = "Tweet sentiment score") +
   ggtitle('b.') #+
  #scale_y_continuous(limits = c(-2, 2))
png("../outputs/posnegss_points_uk.png",units="in", width=8.5, height=10, res=300)
p1/ p + plot_layout(widths = c(1, 1))
dev.off()
```

5 categories Neu (0), S Neg (1), Neg (2), Pos (3), S Pos (4)
```{r}
p2 <- df %>% dplyr::filter(over_tweet_sent < -.05 | over_tweet_sent > .05) %>% 
  ggplot(aes(x = hour, y = over_tweet_sent, colour = as.factor(pn5c_sent))) +
  geom_hline(yintercept = 0, linetype="dashed", color = "red", size = .3) +
  geom_point(colour = "gray", alpha = 0.3, size = 1, shape=".") + 
  geom_smooth(aes(weight = retweet_count), method = "loess", se = TRUE, size=2, span = 0.3) +
  scale_color_manual(name = "Sentiment score", 
                     labels = c("Strongly Negative", "Negative", "Positive", "Strongly Positive"), 
                     values = c("darkred", "#d7191c", "#2c7bb6", "darkblue")) +
#  facet_wrap(~ cntry_id, nrow = 5) + 
  theme_tufte() + 
  theme(text = element_text(size = 20)) +
  theme(legend.position = "bottom") +
  labs(x= "Day",
       y = "Tweet sentiment score") #+
  #scale_y_continuous(limits = c(-2, 2))
png("../outputs/posneg5ss_points_uk.png",units="in", width=9, height=10, res=300)
p1 / p2
dev.off()
```

## 3.8 Word Cloud Analysis

Joining data to add lexicon like text
```{r}
tweets2 <- read_csv("../data/uk_tweets_01122019_01052020.csv") %>% 
  dplyr::select(status_id, lexicon_text)

wc_df <- left_join(wdf, tweets2, by = c("status_id" = "status_id"))
```


```{r}
word_count <- function(x) {
  data(stop_words)
  misc_words <- tibble(word = c("it's", "i've", "lot", "set", "he's", "rt", "-"))
  x <- tibble(id = 1:nrow(x), text = x$lexicon_text)
  x$text <- gsub("[0-9]+", "", x$text)
  x <- x %>% unnest_tokens(word, text)
  x <- x %>% anti_join(stop_words)
  x <- x %>% anti_join(misc_words)
  x <- x %>% dplyr::count(word, sort = TRUE)
}

word_cloud <- function(x, y, z) {
  x <- wordcloud(words = x$word, freq = x$n, min.freq = y,
                 scale=c(5,.2),
          max.words = z, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "RdBu"))
}

filter_date <- function(x,y,z) {
  x <- x %>% dplyr::filter(between(date, as.Date(y), as.Date(z) ) )
}
```

Breakdown word clouds into positive and negative

So the first value y is the minimum number of times a word must occur across all tweets to be considered. The second z is the max number of words to be included in the word cloud

January 15-16th 2020 
> Tweet containing most freq word:
NEGATIVE
* Sick:  Illegal alien Reeaz Khan was arrested for assaulting his grandfather  ICE tried to deport him  NYPD released him onto the streets of Sanctuary City New York instead  6 weeks laterâ€”he's being charged with raping &amp; killing a 92 year old woman  How do Democrats defend this?
* Today I visited 2 Atlanta safe havens for human trafficking survivors.  The warrior women I met today represent resilience and strength on a level few of us will ever know.  The Trump Administration sees and hears you. We are committed to ending the evil of modern slavery. http://url_removed
* China poses an â€œexistential threatâ€ to the international human rights system, according to a new report released today by Human Rights Watch (@anonymous after the organizationâ€™s executive director was denied entry to Hong Kong at the weekend. http://url_removed
* Priti Patel gets it  "The people expect us to do what we say, from crime to immigration, to leaving the EU. We are ready to listen and do what they want.  It's called democracy.  They are our masters and we are their servants  Our job is to deliver on their priorities." http://url_removed
POSITIVE
*	BREAKING   Leave Means Leave have been given approval to hold an event in Parliament Square on 31st January. Great news!  It is a big moment in the history of this nation to celebrate.
* Like me, you may have been wondering about the happy EU millennials in this video who smilingly welcome the @anonymous benign new immigration regime. They really get around these EU nationals. Here they are advertising a bank in Australia: http://url_removed (https://twitter.com/BorderIrish/status/1078352682216235008?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1078352682216235008%7Ctwgr%5E&ref_url=https%3A%2F%2Feuropestreet.news%2Fcontroversy-mounts-over-uk-settled-status-residence-scheme-to-open-to-all-eu-nationals-on-january-21%2F)
* CNN is now doing play-by-play analysis of Bernie &amp; Warren talking after the debate.  They did not bring up gun control, LGBTQIA+ rights, abortion, immigration, wages, police accountability/criminal justice or the Supreme Court among many, many other issues during the debate.

January 22-23th 2020 
> Tweet containing most freq word:
NEGATIVE
* BREAKING: The State Department will no longer issue visas to pregnant immigrants traveling to the United States to give birth and abuse our birthright citizenship laws.
* Immigrant kids were tied to chairs with bags over their head as punishment for misbehaving in custody. Some peed themselves while tied to the chairs.   This is how our country treats immigrant children.  http://url_removed
* BREAKING: WE JUST EXPOSED ILLEGAL VOTING IN ARIZONA.  AZ Blue 2020, a Democratic action group, was caught by our undercover journalist @anonymous SHOWING â€˜illegalsâ€™ how to vote!  THIS NEEDS TO BE SHARED AT ALL LEVELS OF MEDIA!  FULL VIDEO HERE: http://url_removedhttp://url_rem
POSITIVE
* Met #brexit lady again today. She's peed off. Had to apply for residency, register for tax, matriculate cars, join healthcare system, swap driving licence etc, all at a cost. Not what she voted for, she just wanted to stop EU immigration to UK, not stop her living in Portugal.
* I like to think (imagine, hope) that I can comprehend why people take a wide range of political positions, left and right; but I cannot get my head around this. â€˜Unaccompanied child refugees being able to join their families ...â€™ Isnâ€™t that just obvious? Am I missing something? http://url_removed
* After GE2019, Boris Johnson said he wanted to be â€œfriendsâ€ with Remainers.  He has now rejected the Erasmus+ and Child Refugee amendments.  Accepting either or both of those would have been such a positive gesture of friendship &amp; collaboration.  He chose to crush them instead.

January 31st 2020 
> Tweet containing most freq word:
NEGATIVE
* Early in his Administration, President @anonymous committed to bringing the full force and weight of the U.S. Government to tackle this horrific problem.   Since then, he has signed 9 pieces of legislation into law that directly address human trafficking. http://url_removed
* Breaking News: The Trump administration is virtually blocking immigration from Nigeria, Africa's most populous country, and Myanmar, where refugees are fleeing genocide. 4 other countries have also been added in an extension of the stringent travel ban. http://url_removed
* Human trafficking is believed to be one of the largest criminal activities in the worldâ€”with an estimated 24.9 MILLION people trapped in forced labor, domestic servitude, or commercial sex trafficking. http://url_removed
POSITIVE
* Happy Brexit Day!
* RT @anonymous Read this. And next time you see a migrant worker you may think differently. Fantastic article
* President @anonymous just signed an Executive Order on Combating Human Trafficking &amp; Online Child Exploitation.   We will not rest until we have stopped every last human trafficker and liberated every last survivor. http://url_removed

February 3rd-8th 2020 
> Tweet containing most freq word:
NEGATIVE
* dated 2020-02-08: RT @anonymous WATCH: @anonymous tells @anonymous it was a 'mistake' for Leave to claim there'd be Â£350M a week for NHS http://url_removed 
* dated 2020-02-07: Fact:  An illegal immigrant was indicted for the brutal murder of 12 elderly women across Dallas  He's now being accused of at least 10 more killings  This is one of the most brutal serial killings in TX history  I wonder why the media won't cover it?  RT so they can't ignore!
* dated 2020-02-03: If after watching this half-time show, youâ€™re against immigration and think Latinos are invaders......go to boring, rhythm-less hell. Really.  Hispanics are Americans and Make America Great.  Deal with it! ðŸ”¥ - https://eu.azcentral.com/story/opinion/op-ed/elviadiaz/2020/02/03/shakira-and-jlo-brought-latino-pride/4647547002/
* dated 2020-02-05: The shooter at my high school was not an illegal immigrant, he was a 19 year old that was able to go out and LEGALLY buy a weapon of mass destruction.   #SOTU
* dated 2020-02-05: President Donald Trump has been acquitted. The American people have won against the swamp yet again.   The whole impeachment process was grubby, nasty and bad for the USA.
* dated 2020-02-07: No, Mr. President, you &amp; your family did not go through hell. You didnâ€™t even go through a real trial. Hell is what you put immigrant families through when you snatched them apart and put children in cages.
* dated 2020-02-08: My heart breaks for their father, who brought his boys to the US from the Soviet Union because he didnâ€™t want them to live under an authoritarian regime that punished decency in public life. As a fellow Soviet Jewish refugee, I find this particularly chilling. http://url_removed
POSITIVE
* Brexit Party leader @anonymous hasnâ€™t written a manifesto so weâ€™ve done it for him, based on statements by him and his candidates. Billboards going up across the country this week. See more at http://url_removed(location: Radford Rd, Coventry) http://url_removed
* Guys really appreciate it if you rt this. Johnson is going ahead to deport around 50 brits to Jamaica on 11th Feb. Many of these people have lived in UK since childhood. We can't let him do this without making a stand or we are slipping into a nightmare  http://url_removed
* Just had a great meeting with President Trump in the Oval Office.   Good to see the bust of Winston Churchill, there should be great things ahead for our two countries.

# Negative
exp <- wc_df %>% dplyr::filter(between(
  date, as.Date("2020-02-03"),
  as.Date("2020-02-08"))) %>%
  dplyr::filter(over_tweet_sent < -.05) %>%
  dplyr::filter(retweet_count > 199) %>% 
  .[with(., order(retweet_count, over_tweet_sent)),] %>%
 dplyr::filter(retweet_count == 6666)

table(exp$retweet_count)

# Positive
exp <- wc_df %>% dplyr::filter(between(
  date, as.Date("2020-02-03"),
  as.Date("2020-02-08"))) %>%
  dplyr::filter(over_tweet_sent > .05) %>%
  dplyr::filter(retweet_count > 199) %>% 
  .[with(., order(retweet_count, over_tweet_sent)),] %>%
 dplyr::filter(retweet_count == 8500)

table(exp$retweet_count)

```{r, fig.width=14}
wc_df1 <- wc_df %>% dplyr::filter(between(
  date, as.Date("2020-01-15"),
  as.Date("2020-01-16"))) %>% 
  dplyr::filter(over_tweet_sent < -.05) %>% 
  word_count()

wc_df2 <- wc_df %>% dplyr::filter(between(
  date, as.Date("2020-01-15"),
  as.Date("2020-01-16"))) %>% 
  dplyr::filter(over_tweet_sent > .05) %>% 
  word_count()

png("../outputs/wcloud_15-16jan.png",units="in", width=14, height=10, res=300)
#create two panels to add word clouds
par(mfrow = c(1,2))
# create word cloud - negative SSs
wordcloud(wc_df1$word, wc_df1$n, 
                     min.freq =200, 
                     scale=c(5, .1), 
                     random.order = FALSE, 
                     random.color = FALSE, 
                     colors= c("indianred1","indianred2","indianred3","darkred"),
          main="Negative")

# create word cloud - positive SSs
wordcloud(wc_df2$word, wc_df2$n, 
          min.freq =200, 
          scale=c(5, .1), 
          random.order = FALSE, 
          random.color = FALSE, 
          colors=c("lightsteelblue1", "lightsteelblue2","#2c7bb6","darkblue"))
dev.off()

```


Jan 22-23rd 2020 

```{r, fig.width=14}
wc_df1 <- wc_df %>% dplyr::filter(between(
  date, as.Date("2020-01-22"),
  as.Date("2020-01-23"))) %>% 
  dplyr::filter(over_tweet_sent < -.05) %>% 
  word_count()

wc_df2 <- wc_df %>% dplyr::filter(between(
  date, as.Date("2020-01-22"),
  as.Date("2020-01-23"))) %>% 
  dplyr::filter(over_tweet_sent > .05) %>% 
  word_count()

png("../outputs/wcloud_22-23jan.png",units="in", width=14, height=10, res=300)
#create two panels to add word clouds
par(mfrow = c(1,2))
# create word cloud - negative SSs
wordcloud(wc_df1$word, wc_df1$n, 
                     min.freq =200, 
                     scale=c(5, .1), 
                     random.order = FALSE, 
                     random.color = FALSE, 
                     colors= c("indianred1","indianred2","indianred3","darkred"))

# create word cloud - positive SSs
wordcloud(wc_df2$word, wc_df2$n, 
          min.freq =200, 
          scale=c(5, .1), 
          random.order = FALSE, 
          random.color = FALSE, 
          colors=c("lightsteelblue1", "lightsteelblue2","#2c7bb6","darkblue"))
dev.off()

```

February 3rd-8th 2020 

```{r, fig.width=14}
wc_df1 <- wc_df %>% dplyr::filter(between(
  date, as.Date("2020-02-03"),
  as.Date("2020-02-08"))) %>% 
  dplyr::filter(over_tweet_sent < -.05) %>% 
  word_count()

wc_df2 <- wc_df %>% dplyr::filter(between(
  date, as.Date("2020-02-03"),
  as.Date("2020-02-08"))) %>% 
  dplyr::filter(over_tweet_sent > .05) %>% 
  word_count()

png("../outputs/wcloud_3-8feb.png",units="in", width=14, height=10, res=300)
#create two panels to add word clouds
par(mfrow = c(1,2))
# create word cloud - negative SSs
wordcloud(wc_df1$word, wc_df1$n, 
                     min.freq =200, 
                     scale=c(5, .1), 
                     random.order = FALSE, 
                     random.color = FALSE, 
                     colors= c("indianred1","indianred2","indianred3","darkred"))

# create word cloud - positive SSs
wordcloud(wc_df2$word, wc_df2$n, 
          min.freq =200, 
          scale=c(5, .1), 
          random.order = FALSE, 
          random.color = FALSE, 
          colors=c("lightsteelblue1", "lightsteelblue2","#2c7bb6","darkblue"))
dev.off()

```







