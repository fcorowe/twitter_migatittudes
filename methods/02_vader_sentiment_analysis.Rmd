---
title: "Twitter Data Analysis - VADER"
author: "F Rowe"
output: html_notebook
---

```{r}
rm(list=ls())
```


# 1. Library necessary packages
```{r}
library(rtweet)
library(tidyverse)
library(tidytext)
library(textdata)
library(dplyr)
library(stringr)
library(tidyr)
library(plyr)
library(lubridate)
library(ggplot2)
library(RColorBrewer)
library(gridExtra)
library(sentimentr)
library(ggthemes)
library(showtext)
library(wordcloud)
library(vader)
library(patchwork)
library(viridis)
```

Set font style
```{r}
# load font
font_add_google("Roboto Condensed", "robotocondensed")
# automatically use showtext to render text
showtext_auto()
```


# 2. Data Wrangling
```{r}
# Read in tweets
tweets <- read_csv("../data/uk_tweets_01122019_01052020_VADER.csv")
glimpse(tweets)
```
The code chunk above reads a data frame with following structure:

```
Rows: 108,114
Columns: 13
$ X1             <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …
$ created_at     <dttm> 2019-12-01 00:09:13, 2019-12-01 01:06:54, 2019-12-01 01:10:31…
$ status_id      <dbl> 1.200930e+18, 1.200944e+18, 1.200945e+18, 1.200946e+18, 1.2009…
$ username       <chr> "Computer_999", "Bethalyza", "CGH54", "sunnydalejane", "Katcam…
$ user_id        <dbl> 4.243425e+08, 3.694850e+08, 3.847631e+08, 7.818855e+17, 4.9541…
$ text           <chr> "RT @xxx: If #NigelFarage wants to leave the EU, this …
$ country        <chr> "United Kingdom", "United Kingdom", "United Kingdom", NA, "Uni…
$ region         <chr> "England", "England", "Wales", NA, NA, "England", "England", N…
$ retweeted_user <chr> "{'user_id': '2196203149', 'username': 'xxx'}", NA, NA…
$ retweet_count  <dbl> 34, 0, 0, 1, 3, 868, 0, 0, 149, 237, 853, 9, 16, 4, 0, 0, 131,…
$ search_type    <chr> "hashtags", "hashtags", "hashtags", "hashtags", "hashtags", "h…
$ quoted_tweet   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…
$ VADER_text     <chr> "RT @anonymous If #NigelFarage wants to leave the EU, this app…
```

Formatting dates
```{r}
# split string character date
date_df <- tweets$created_at %>% 
  str_split(" ", simplify = TRUE) %>% 
  as.data.frame()
date_df %>% head(5)

# create date variable
tweets$date <- ymd(date_df$V1)
  
rm(date_df)
```

```{r}
#remove
tweets$X1 <- NULL

# country id
tweets$cntry_id <- 1 # uk

```


## Select sample for this analysis
```{r}
tweets <- tweets %>% dplyr::filter(between(
  date, as.Date("2020-01-15"),
  as.Date("2020-02-15")
  )
  )
```

# 3. Sentiment Analysis

## 3.1. Getting the scores
```{r}
vader_sentiment <- vader_df(tweets$VADER_text)
```

Computing summary metrics
```{r}
#join data
df <- cbind(tweets$date, vader_sentiment, tweets$retweet_count, tweets$quoted_tweet, tweets$created_at, tweets$status_id)

df <- df %>% dplyr::rename(date = `tweets$date`,
                retweet_count = `tweets$retweet_count`, 
                quoted_tweet = `tweets$quoted_tweet`,
                over_tweet_sent = compound,
                hour = `tweets$created_at`,
                status_id = `tweets$status_id`)


# day level sentiment score - check
day_sent <- df %>% 
  group_by(date) %>% 
  dplyr::summarize(
    wave_sent = weighted.mean(over_tweet_sent, retweet_count),
    ave_sent = mean(over_tweet_sent)
    )

# add vars
df <- df %>% 
  mutate(pn_sent = 
           case_when(
           over_tweet_sent == 0 ~ 0,
           over_tweet_sent > 0 ~ 1,
           over_tweet_sent < 0 ~ 2
           ), # id for neutral, positive and negative sentiment
         pn5c_sent =
           case_when(
           over_tweet_sent >= -.05 & over_tweet_sent <= .05 ~ 3,
           over_tweet_sent < -.5 ~ 1,
           over_tweet_sent < -.05 & over_tweet_sent >= -.5 ~ 2,
           over_tweet_sent > .05 & over_tweet_sent <= .5 ~ 4,
           over_tweet_sent > .5 ~ 5,
           )
         )
```

Expanding the dataset 
```{r}
# create function to expand the data
apply_weightings <- function (x) {
  x <- x[rep(row.names(x), x$retweet_count), c('date', 'text','over_tweet_sent','retweet_count','status_id', 'hour')]
}

wdf <- apply_weightings(df)
```


## 3.2 Total number of tweets
```{r}
sum(df$retweet_count)
```

## 3.3. Distribution Analysis: Creating Fig.1

Histogram
```{r}
p1 <- ggplot(data=df) +
  geom_histogram(aes(x = over_tweet_sent, 
                   weight = retweet_count/sum(retweet_count)), 
                 binwidth = 0.05,
                 fill = "#440154FF",
                 alpha = 1) +
  #facet_grid(. ~ cntry_id) +
  theme_tufte() + 
  theme(text = element_text(family="robotocondensed",
                            size = 20)) +
  labs(x= "Tweet sentiment score",
       y = "Density")
```

Cumulative distribution
```{r}
p2 <- ggplot(wdf, aes(over_tweet_sent)) + 
  stat_ecdf(geom = "step",
            size = 2,
            colour = "#440154FF") +
  theme_tufte() + 
  theme(text = element_text(family="robotocondensed",
                            size = 20)) +
  labs(x= "Tweet sentiment score",
       y = "Cumulative density")
```
Saving Fig.1
```{r}
png("../outputs/oss_dist_uk.png", units="in", width=10, height=7, res=300)
p1 + p2 + plot_layout(widths = c(1, 1))
dev.off()
```

## 3.4. Temporal evolution: Creating Fig.2

### Creating Fig.2b

Frequency Neu (0), Pos (1), Neg (2)
```{r}
dplyr::count(x = df, pn_sent, wt = retweet_count)
```

Totals - 5 categories Neu (3), S Neg (1), Neg (2), Pos (4), S Pos (5),
```{r}
dplyr::count(x = df, pn5c_sent, wt = retweet_count)
```

Daily Frequency Neu (0), Pos (1), Neg (2)
```{r}
tb_counts <- df %>% dplyr::count(date, pn_sent, wt = retweet_count) %>% 
  spread(pn_sent, n)
tb_counts
```

Daily Share Neu (0), Pos (1), Neg (2)
```{r}
tb_counts[,2:4] / rowSums(tb_counts[,2:4]) * 100
```

Daily Frequency - 5 categories Neu (3), S Neg (1), Neg (2), Pos (4), S Pos (5)
```{r}
tb_counts5 <- df %>% dplyr::count(date, pn5c_sent, wt = retweet_count) %>% 
  spread(pn5c_sent, n)
tab <- cbind(tb_counts5[,1], (tb_counts5[,2:6] / rowSums(tb_counts5[,2:6]) * 100)) %>% 
  dplyr::rename(day = `tb_counts5[, 1]`) %>% 
  .[, c(1, 3, 4, 2, 5, 6)] %>% 
  gather(stance, percent, -day)

pcom <- ggplot(tab, aes(fill = stance, y=percent, x=day)) + 
    geom_bar(position="stack", stat="identity") + 
    theme_tufte() + 
    theme(text = element_text(family="robotocondensed",
                            size = 20),
          legend.position = "bottom") +
    scale_fill_manual(values = c("darkred","#d7191c", "#f7f7f7", "#2c7bb6", "darkblue"),
                      labels = c("Strongly Negative", "Negative", "Neutral", "Positive", "Strongly Positive")) +
  labs(x= "Day",
       y = "Percent") +
   ggtitle('b.')
    #scale_fill_viridis_d()
```

### Creating Fig.2a

SS Daily evolution - smoothed conditional mean

```{r}
p1 <- ggplot(df, aes(x = hour, y = over_tweet_sent)) +
 geom_point(colour = "gray", alpha = 0.3, size = 1, shape=".") + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "red", size = .3) +
  geom_smooth(aes(weight = retweet_count), method = "loess", se = TRUE, size=2, span = 0.3, color="#440154FF") +
# facet_wrap(~ cntry_id, nrow = 5) + 
  theme_tufte() + 
  theme(text = element_text(family="robotocondensed",
                            size = 20)) +
  labs(x= "Day",
       y = "Tweet sentiment score")  +
   ggtitle('a.') +
  scale_y_continuous(limits = c(-1, 1))
```
Saving Fig.2
```{r}
png("../outputs/oss_points_uk.png", units="in", width=10, height=10, res=300)
p1 / pcom + plot_layout(widths = c(1, 1))
dev.off()
```

## 3.5 Word Cloud Analysis: Creating Fig.3

Joining data to add lexicon like text
```{r}
tweets2 <- read_csv("../data/uk_tweets_01122019_01052020.csv") %>% 
  dplyr::select(status_id, lexicon_text)

wc_df <- left_join(wdf, tweets2, by = c("status_id" = "status_id"))
```


```{r}
word_count <- function(x) {
  data(stop_words)
  misc_words <- tibble(word = c("it's", "i've", "lot", "set", "he's", "rt", "-"))
  x <- tibble(id = 1:nrow(x), text = x$lexicon_text)
  x$text <- gsub("[0-9]+", "", x$text)
  x <- x %>% unnest_tokens(word, text)
  x <- x %>% anti_join(stop_words)
  x <- x %>% anti_join(misc_words)
  x <- x %>% dplyr::count(word, sort = TRUE)
}

word_cloud <- function(x, y, z) {
  x <- wordcloud(words = x$word, freq = x$n, min.freq = y,
                 scale=c(5,.2),
          max.words = z, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "RdBu"))
}

filter_date <- function(x,y,z) {
  x <- x %>% dplyr::filter(between(date, as.Date(y), as.Date(z) ) )
}
```

Breakdown word clouds into positive and negative

Fig.3a: Jan 15-16th, 2020
```{r, fig.width=14}
wc_df1 <- wc_df %>% dplyr::filter(between(
  date, as.Date("2020-01-15"),
  as.Date("2020-01-16"))) %>% 
  dplyr::filter(over_tweet_sent < -.05) %>% 
  word_count()

wc_df2 <- wc_df %>% dplyr::filter(between(
  date, as.Date("2020-01-15"),
  as.Date("2020-01-16"))) %>% 
  dplyr::filter(over_tweet_sent > .05) %>% 
  word_count()

png("../outputs/wcloud_15-16jan.png",units="in", width=14, height=10, res=300)
#create two panels to add word clouds
par(mfrow = c(1,2))
# create word cloud - negative SSs
wordcloud(wc_df1$word, wc_df1$n, 
                     min.freq =200, 
                     scale=c(5, .1), 
                     random.order = FALSE, 
                     random.color = FALSE, 
                     colors= c("indianred1","indianred2","indianred3","darkred"),
          main="Negative")

# create word cloud - positive SSs
wordcloud(wc_df2$word, wc_df2$n, 
          min.freq =200, 
          scale=c(5, .1), 
          random.order = FALSE, 
          random.color = FALSE, 
          colors=c("lightsteelblue1", "lightsteelblue2","#2c7bb6","darkblue"))
dev.off()

```


Fig.3b: Jan 22-23rd 2020 

```{r, fig.width=14}
wc_df1 <- wc_df %>% dplyr::filter(between(
  date, as.Date("2020-01-22"),
  as.Date("2020-01-23"))) %>% 
  dplyr::filter(over_tweet_sent < -.05) %>% 
  word_count()

wc_df2 <- wc_df %>% dplyr::filter(between(
  date, as.Date("2020-01-22"),
  as.Date("2020-01-23"))) %>% 
  dplyr::filter(over_tweet_sent > .05) %>% 
  word_count()

png("../outputs/wcloud_22-23jan.png",units="in", width=14, height=10, res=300)
#create two panels to add word clouds
par(mfrow = c(1,2))
# create word cloud - negative SSs
wordcloud(wc_df1$word, wc_df1$n, 
                     min.freq =200, 
                     scale=c(5, .1), 
                     random.order = FALSE, 
                     random.color = FALSE, 
                     colors= c("indianred1","indianred2","indianred3","darkred"))

# create word cloud - positive SSs
wordcloud(wc_df2$word, wc_df2$n, 
          min.freq =200, 
          scale=c(5, .1), 
          random.order = FALSE, 
          random.color = FALSE, 
          colors=c("lightsteelblue1", "lightsteelblue2","#2c7bb6","darkblue"))
dev.off()

```

Fig.3c: February 3rd-8th 2020 

```{r, fig.width=14}
wc_df1 <- wc_df %>% dplyr::filter(between(
  date, as.Date("2020-02-03"),
  as.Date("2020-02-08"))) %>% 
  dplyr::filter(over_tweet_sent < -.05) %>% 
  word_count()

wc_df2 <- wc_df %>% dplyr::filter(between(
  date, as.Date("2020-02-03"),
  as.Date("2020-02-08"))) %>% 
  dplyr::filter(over_tweet_sent > .05) %>% 
  word_count()

png("../outputs/wcloud_3-8feb.png",units="in", width=14, height=10, res=300)
#create two panels to add word clouds
par(mfrow = c(1,2))
# create word cloud - negative SSs
wordcloud(wc_df1$word, wc_df1$n, 
                     min.freq =200, 
                     scale=c(5, .1), 
                     random.order = FALSE, 
                     random.color = FALSE, 
                     colors= c("indianred1","indianred2","indianred3","darkred"))

# create word cloud - positive SSs
wordcloud(wc_df2$word, wc_df2$n, 
          min.freq =200, 
          scale=c(5, .1), 
          random.order = FALSE, 
          random.color = FALSE, 
          colors=c("lightsteelblue1", "lightsteelblue2","#2c7bb6","darkblue"))
dev.off()
```







